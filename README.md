# ðŸ¦™ llama.rs

Rust workspace for building a modular integration and runtime layer around `llama.cpp`.

## Workspace Structure

```
llama.rs/
â”œâ”€â”€ llama-core/      # (Optional) Rust-native tensor and memory operations (ggml-style)
â”œâ”€â”€ llama-sys/       # FFI bindings to C/C++ functions from llama.cpp (via `cc`)
â”œâ”€â”€ llama-model/     # Responsible for GGUF parsing, tokenizer, vocabulary loading
â”œâ”€â”€ llama-runtime/   # High-level execution API, wraps llama-core + model loading
â”œâ”€â”€ llama-cli/       # Example CLI application that runs inference
â””â”€â”€ examples/        # (Optional) Additional Rust examples and integration tests
```

## Getting Started

```bash
cargo build --workspace
cargo run -p llama-cli
```

## License

Apache 2.0 License
